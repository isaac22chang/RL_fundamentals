{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c958dfc-8f5b-411f-8181-ef41b699b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy based reinforcement learning allows agents to learn based on current policy instead of learning action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da4000d-193c-4e98-ac2f-91ea8b0b8431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages needed\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0214855b-d835-4c85-aac3-fba0924cf63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   1%|▊                                                                                      | 1/100 [00:00<00:29,  3.41ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Avg Reward: 28.00 | Episodes: [19.0, 32.0, 12.0, 45.0, 32.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  21%|██████████████████                                                                    | 21/100 [00:05<00:24,  3.19ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 | Avg Reward: 48.20 | Episodes: [37.0, 39.0, 41.0, 28.0, 96.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  41%|███████████████████████████████████▎                                                  | 41/100 [00:18<00:47,  1.25ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40 | Avg Reward: 87.20 | Episodes: [106.0, 125.0, 90.0, 62.0, 53.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  61%|████████████████████████████████████████████████████▍                                 | 61/100 [00:52<01:35,  2.44s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60 | Avg Reward: 289.60 | Episodes: [401.0, 332.0, 255.0, 199.0, 261.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  81%|█████████████████████████████████████████████████████████████████████▋                | 81/100 [01:43<00:40,  2.15s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80 | Avg Reward: 224.40 | Episodes: [257.0, 249.0, 217.0, 191.0, 208.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:24<00:00,  1.44s/ep]\n"
     ]
    }
   ],
   "source": [
    "#Define policy\n",
    "class VPG(nn.Module):\n",
    "    def __init__(self,state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits = logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "#Define training loop\n",
    "def get_rewards_tg(rewards, gamma = 0.99):\n",
    "    discounted_return = 0\n",
    "    rewards_to_go = []\n",
    "    for r in reversed(rewards):\n",
    "        discounted_return = r + gamma * discounted_return\n",
    "        #What we're doing at each step is adding the current reward to all previous rewards, which are multiplied by a factor to represent reward decay \n",
    "        rewards_to_go.insert(0, discounted_return) \n",
    "    return torch.tensor(rewards_to_go)\n",
    "    \n",
    "    \n",
    "    \n",
    "epochs = 100\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "policy = VPG(state_size,action_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 0.003)\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "for episode in tqdm(range(epochs), desc = \"Training Epochs: \", unit = \"ep\"):\n",
    "    \n",
    "\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "    batch_episode_rewards = []\n",
    "\n",
    "    #We want to get a batch of episodes before updating the model due to high variability of episodes.\n",
    "    #This reduces noise of very lucky and unlucky episodes that can mess up agent weights.\n",
    "    for _ in range(batch_size):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "\n",
    "    #Gather results from one entire episode (different than DQN where each action makes the agent reevaluate)\n",
    "        while not done:\n",
    "            action, log_prob = policy.get_action(state)\n",
    "            \n",
    "            next_state, reward, termination, truncation, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            done = termination or truncation\n",
    "    \n",
    "            #NOTE: Don't change next_state into FloatTensor because get_action() expects a numpy array\n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        rtg = get_rewards_tg(rewards)\n",
    "        \n",
    "\n",
    "        #Update our batch with this episode's log_probs and rewards\n",
    "        batch_log_probs.extend(log_probs)\n",
    "        batch_rewards.extend(rtg)    \n",
    "        batch_episode_rewards.append(sum(rewards))\n",
    "\n",
    "\n",
    "    batch_log_probs = torch.stack(batch_log_probs).squeeze()\n",
    "    batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32)\n",
    "\n",
    "    #Currently, in our batch of rewards, the rewards are just a scalar value, and don't actually hold any representation of how good an action was given the associated reward\n",
    "    #By standardizing the reward values, we can actually distinguish between good and bad actions\n",
    "    \n",
    "    batch_rewards = (batch_rewards - batch_rewards.mean()) / (batch_rewards.std() + 1e-9)  #The offset of 1e-9 is a failsafe in case standard deviation is 0\n",
    "\n",
    "    #Update weights:\n",
    "    #After gathering results and rewards, we want to update the agent with policy gradient theorem\n",
    "    #The policy gradient theorem = average value (E) of [summation from 0 to T of policy function * Advantage]\n",
    "    #Whole equation can be found here: https://spinningup.openai.com/en/latest/algorithms/vpg.html#background    \n",
    "    #Due to advantage function needing something beyond VPG representation, we need a substitution for the advantage function, using rewards to go (rtg)\n",
    "    #By taking the mean of rtg * log_probs, we can represent the policy gradient theorem\n",
    "    #Why -rtg*log_probs? Since loss.backward() performs gradient descent, we need gradient ascent in order to maximize reward, so we reverse the direction the gradient propagates\n",
    "    #detach prevents \n",
    "    \n",
    "    loss = -(batch_log_probs * batch_rewards).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "    if episode % 20 == 0:\n",
    "        avg_reward = sum(batch_episode_rewards) / batch_size  # ← CHANGE THIS\n",
    "        print(f\"Episode {episode} | Avg Reward: {avg_reward:.2f} | Episodes: {batch_episode_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20b13081-bfc1-4627-90bd-cff88a0f9ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\isaac\\Projects\\RL\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: 331.0 steps\n",
      "Episode 2: 118.0 steps\n",
      "Episode 3: 500.0 steps\n",
      "Episode 4: 405.0 steps\n",
      "Episode 5: 413.0 steps\n",
      "Videos saved to ./videos/\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# After training, wrap your environment with RecordVideo\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")  # Note: rgb_array, not human\n",
    "test_env = RecordVideo(\n",
    "    test_env, \n",
    "    video_folder=\"./videos\",  # Where to save videos\n",
    "    episode_trigger=lambda x: True,  # Record every episode\n",
    "    name_prefix=\"vpg-cartpole-vpg\"\n",
    ")\n",
    "\n",
    "for episode in range(5):\n",
    "    state, _ = test_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = policy.get_action(state)\n",
    "        state, reward, termination, truncation, _ = test_env.step(action)\n",
    "        total_reward += reward\n",
    "        done = termination or truncation\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: {total_reward} steps\")\n",
    "\n",
    "test_env.close()\n",
    "print(\"Videos saved to ./videos/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3072a8-ff34-4b10-b881-714225b55f1b",
   "metadata": {},
   "source": [
    "Errors I ran into when coding implementation:\n",
    "- Converting next_state into a tensor, when get_action expects a numpy array. Make sure to keep nparray and tensor data types consistent.\n",
    "- When doing batches of epsiodes, you can't just collect 10 episodes worth of rewards and log_probs, and perform loss on that.\n",
    "- This assumes that all rewards and log_probs are one episode. To actually implement episode batching, you need to store them in a bucket.\n",
    "\n",
    "Dealing with low reward outcomes (from most effective to least):\n",
    "- Batching episodes increased performance drastically. The reasoning here is that episodes are highly variable, so updating the model weights per episode can lead to inefficient performance. By batching, we can have better training runs where the impact of outlier episodes are relatively mitigated.\n",
    "Gradient clipping means that the agent policy won't be subject to large updates, which can happen when gradients become very large due to extreme log probabilities. For instance, if a low probability action is sampled from the categorical distribution of logits, it will generate high gradient values, which can cause the model to update drastically due to log(small #) being of large magnitude. This would cause unlikely actions to disproportionally impact our model learning.\n",
    "- Learning rates (LR) are dependent on episode batch size. For this case, with no batching, a lower LR performed slightly better than high LR, but with batching, a higher LR performed better.  (For 100 batches of 5 episodes, the agent went from averaging 0.4 seconds / batch with LR of 0.003 to 3.5 seconds/batch with LR of 0.01 (this is with no human rendering time overhead)).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
