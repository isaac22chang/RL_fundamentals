{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13dd5018-0d96-428e-bbd6-c74e16171ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages needed\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6678317-6239-4000-9c81-45f04363bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define value and policy network\n",
    "\n",
    "#Value network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,1) #1 is the expected reward\n",
    "        )\n",
    "    def forward(self, state):\n",
    "        value = self.net(state)\n",
    "        return value.squeeze(-1) \n",
    "\n",
    "#Policy network\n",
    "class VPG(nn.Module):\n",
    "    def __init__(self,state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,action_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return logits\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        logits = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits = logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "        \n",
    "def get_rewards_tg(rewards, gamma = 0.99):\n",
    "    discounted_return = 0\n",
    "    rewards_to_go = []\n",
    "    for r in reversed(rewards):\n",
    "        discounted_return = r + gamma * discounted_return\n",
    "        #What we're doing at each step is adding the current reward to all previous rewards, which are multiplied by a factor to represent reward decay \n",
    "        rewards_to_go.insert(0, discounted_return) \n",
    "    return torch.tensor(rewards_to_go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00d40e52-c72a-4082-a4e8-2a863f768a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   2%|█▎                                                               | 2/100 [00:00<00:38,  2.58ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n",
      "  Avg Reward: 18.80\n",
      "  Policy Loss: 0.0094\n",
      "  Value Loss: 1.0190\n",
      "  Mean Value Prediction: 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  21%|█████████████▍                                                  | 21/100 [00:06<00:30,  2.58ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20\n",
      "  Avg Reward: 32.40\n",
      "  Policy Loss: -0.0239\n",
      "  Value Loss: 0.8734\n",
      "  Mean Value Prediction: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  41%|██████████████████████████▏                                     | 41/100 [00:23<01:07,  1.15s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40\n",
      "  Avg Reward: 137.00\n",
      "  Policy Loss: -0.0191\n",
      "  Value Loss: 0.4853\n",
      "  Mean Value Prediction: 0.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  61%|███████████████████████████████████████                         | 61/100 [00:59<01:40,  2.58s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60\n",
      "  Avg Reward: 377.20\n",
      "  Policy Loss: 0.0036\n",
      "  Value Loss: 0.6839\n",
      "  Mean Value Prediction: -0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  81%|███████████████████████████████████████████████████▊            | 81/100 [02:02<00:58,  3.08s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80\n",
      "  Avg Reward: 428.00\n",
      "  Policy Loss: -0.0259\n",
      "  Value Loss: 0.6697\n",
      "  Mean Value Prediction: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|███████████████████████████████████████████████████████████████| 100/100 [03:11<00:00,  1.91s/ep]\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs = 100\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "policy = VPG(state_size,action_size)\n",
    "value = ValueNetwork(state_size)\n",
    "\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr = 0.003)\n",
    "value_optimizer = optim.Adam(value.parameters(), lr = 0.005)\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "for episode in tqdm(range(epochs), desc = \"Training Epochs: \", unit = \"ep\"):\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "    batch_episode_rewards = []\n",
    "    batch_values = []\n",
    "    #update policy with batches of episodes\n",
    "    for _ in range(batch_size):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = policy.get_action(state)\n",
    "            next_state, reward, termination, truncation, _ = env.step(action)\n",
    "            done = termination or truncation\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            states.append(state)\n",
    "    \n",
    "            state = next_state \n",
    "            \n",
    "        rtg = get_rewards_tg(rewards)   \n",
    "        states_tensor = torch.FloatTensor(np.array(states))\n",
    "        expected_value = value(states_tensor)\n",
    "        \n",
    "        batch_log_probs.extend(log_probs)\n",
    "        batch_rewards.extend(rtg)\n",
    "        batch_values.append(expected_value.squeeze()) #Right now expected_value is a \n",
    "        batch_episode_rewards.append(sum(rewards))\n",
    "\n",
    "    batch_log_probs = torch.stack(batch_log_probs).squeeze()\n",
    "    batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32)\n",
    "    batch_values = torch.cat(batch_values)\n",
    "\n",
    "    batch_rewards = (batch_rewards - batch_rewards.mean()) / (batch_rewards.std() + 1e-9)\n",
    "    advantages = batch_rewards - batch_values.detach()\n",
    "    \n",
    "    #normalize advantages? We normalized batch_rewards so I'm assuming we should do that for advantage to\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-9)\n",
    "\n",
    "\n",
    "    #How to update value network?\n",
    "    value_loss = F.mse_loss(batch_values.squeeze(), batch_rewards.squeeze())\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(value.parameters(), max_norm=1.0)\n",
    "\n",
    "    value_optimizer.step()\n",
    "    \n",
    "    #Update policy\n",
    "    policy_loss = -(batch_log_probs * advantages).mean()\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    if episode % 20 == 0:\n",
    "        avg_reward = sum(batch_episode_rewards) / batch_size\n",
    "        print(f\"\\nEpoch {episode}\")\n",
    "        print(f\"  Avg Reward: {avg_reward:.2f}\")\n",
    "        print(f\"  Policy Loss: {policy_loss.item():.4f}\")\n",
    "        print(f\"  Value Loss: {value_loss.item():.4f}\")  # Should decrease over time!\n",
    "        print(f\"  Mean Value Prediction: {batch_values.mean().item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f312b-e3c1-42aa-a3e7-aa54019c1689",
   "metadata": {},
   "source": [
    "Insights/mistakes:\n",
    "\n",
    "To implement vanilla policy gradient (VPG) with baseline, I expanded on the code from the base VPG implementation. I tried to code this without looking at any existing implementations, and I decided to look at Spinning Up RL and online resources to understand the high-level process of adding a value function. \n",
    "\n",
    "After reading, I first added a value network, and then modified my training loop to compute the advantage function using Q(s,a) - V(s)). I originally added an arbitrary lighter neural network for my value network, but this turned out to be not a good decision. Since value functions are supposed to be very good at evaluating values, it's better to have a more dense neural network that can predict values better. This value function would be used along with the existing rewards to go to create the advantage function (Q(s,a) - V(s) where Q(s,a) = rewards to go, and V(s) is estimted value). This is an improvement on base VPG, which represents the advantage function using just Q(s,a)/rewards to go without a value estimator. Doing this subtraction of the baseline reduces variance.\n",
    "\n",
    "\n",
    "When training the updated training loop, I ran into a problem where my value loss was absurdly high. After looking more at the code, I noticed that I didn't normalize the rewards. The problem with this is that a neural network is initialized to predict values near 0. This means it's hard to predict large values [0,500]. By normalizing the rewards, it would make it easier for value function/neural network to predict larger values, which are now \"scaled down\" to a lower range. To elaborate, by normalizing the reward to mean = 0 and std = 1, value network can learn more effectively since the range is smaller.\n",
    "\n",
    "In addition to normalizing rewards, it's also important to normalize advantages so that it makes learning more stable across batches where rewards can differ even in the episodes within each batch.\n",
    "\n",
    "TLDR:\n",
    "- Advantage function can be represented by the value function subtracted from rewards to go, instead of just using rewards to go in base VPG implementation. By having rewards to go minus the baseline (result of value function), we can reduce variance because the model now understands what's better or worse than expected. Rather than just having a rewards number with no comparison.\n",
    "- Have a value function that is of equal or higher complexity than the policy network\n",
    "- Normalize rewards to lower value loss by making larger values more predictable for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e88db-5a4c-4035-8bb2-4a4cb1350452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
