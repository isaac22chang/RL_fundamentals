{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db00118-c66a-4cfc-b074-0b05b6c9a77f",
   "metadata": {},
   "source": [
    "Notebook covers:\n",
    "- Implementation of DQN using Gymnasium's Cart Pole v1 environment\n",
    "- Includes personal notes at each step to increase intuition and understanding of how reinforcement learning works\n",
    "- Timeline and reasoning of DQN Improvements (basic DQN -> implementing replay Buffer -> target network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ebe06f-ff1b-49db-bd8a-6c51a5ccf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed64c15-64e4-4a08-a2ad-b60c207b4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network as model\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim,64), #map # of states to 64 dimensions\n",
    "            nn.ReLU(), #introduce non-linearity by turning all negative values into 0\n",
    "            nn.Linear(64,action_size), #map 64 dimensions to # of actions\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0188610b-8a86-450d-b598-31d25630e709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 1000/1000 [00:34<00:00, 29.14ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time from episode 0-10: 18.4\n",
      "\n",
      "Average Time from episode 100-110: 9.2\n",
      "\n",
      "Average Time from episode 200-210: 11.9\n",
      "\n",
      "Average Time from episode 300-310: 10.0\n",
      "\n",
      "Average Time from episode 400-410: 10.3\n",
      "\n",
      "Average Time from episode 500-510: 9.6\n",
      "\n",
      "Average Time from episode 600-610: 13.3\n",
      "\n",
      "Average Time from episode 700-710: 21.2\n",
      "\n",
      "Average Time from episode 800-810: 34.2\n",
      "\n",
      "Average Time from episode 900-910: 38.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "model = DQN(state_size,action_size)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "gamma = 0.99\n",
    "\n",
    "results = []\n",
    "for episode in tqdm(range(1000), desc=\"Training Episodes\", unit=\"ep\"):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        q_values = model(state)\n",
    "        #Greedy epsilon\n",
    "        if random.uniform(0,1) > 0.1: #Set at 10% chance for exploration\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        next_state, reward, termination, truncation, _ = env.step(action)\n",
    "        done = termination or truncation\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        current_q = model(state)[action]\n",
    "        \n",
    "        #Make sure that we don't update weights of the actual model\n",
    "        with torch.no_grad():\n",
    "            max_next_q = torch.max(model(next_state)) #Given the current choice, we look at the next state to determine if we made the most informed choice\n",
    "            target_q = reward + (gamma * max_next_q * (1 - done)) #Since there's some discrepancy between current state and reality of next state, we update loss to make current closer to truth \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        #Note: Since loss is more about minimizing diff between reality and expected rewards, loss isn't a good rep of how good an agent performs\n",
    "\n",
    "        #Update loss given the information we have\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "    results.append(episode_reward)\n",
    "\n",
    "for i in range(0,1000,100):\n",
    "    avg = sum(results[i:i+10]) / len(results[i:i+10])\n",
    "    print(f\"Average Time from episode {i}-{i+10}: {avg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "959871df-ca35-4918-9c2a-04ecaa3c5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to save a trained agent run into video folder\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "video_env = RecordVideo(video_env, video_folder=\"./videos\", episode_trigger=lambda x: True,name_prefix=\"basic_dqn_cartpole\")\n",
    "\n",
    "state, _ = video_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state)\n",
    "    action = torch.argmax(model(state_tensor)).item()\n",
    "    state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "video_env.close()\n",
    "\n",
    "#Notes:\n",
    "#Watching the agent, the agent just leans to the right or left\n",
    "#Due to sequential correlation, the agent leans to the right instead of trying to balance\n",
    "#One way to address this issue is to use replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f94928-b51a-480c-ab1a-62dc39bd936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "#Implementation of Replay Buffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self,size):\n",
    "        batch = random.sample(self.buffer,size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd6a671-2d4d-410d-a99f-a469e0101cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 1000/1000 [07:37<00:00,  2.19ep/s]\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(10000)\n",
    "batch_size = 32\n",
    "rb_env = gym.make(\"CartPole-v1\")\n",
    "rb_state_size = rb_env.observation_space.shape[0]\n",
    "rb_action_size = rb_env.action_space.n\n",
    "rb_model = DQN(rb_state_size,rb_action_size)\n",
    "rb_optimizer = optim.Adam(rb_model.parameters(),lr = 0.001)\n",
    "gamma = 0.99\n",
    "\n",
    "rb_results = []\n",
    "for episode in tqdm(range(1000), desc=\"Training Episodes\", unit=\"ep\"):\n",
    "    state, _ = rb_env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        q_values = rb_model(state)\n",
    "        if random.uniform(0,1) > 0.1: #Set at 10% chance for exploration\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = rb_env.action_space.sample()\n",
    " \n",
    "        next_state, reward, termination, truncation, _ = rb_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        \n",
    "        done = termination or truncation\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "\n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "        #Check if buffer is greater than batch size\n",
    "        if len(memory) >= batch_size:\n",
    "            #Retrieve 32 (from batch_size) entries from replay buffer\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = memory.sample(batch_size)        \n",
    "\n",
    "            #Stack states so that the neural network can process all states in the batch at one time\n",
    "            batch_state = torch.stack(batch_state)\n",
    "            batch_next_state = torch.stack(batch_next_state)\n",
    "    \n",
    "            #Convert reward and done into tensors for neural network processing\n",
    "            batch_reward = torch.FloatTensor(batch_reward)\n",
    "            batch_action = torch.LongTensor(batch_action)\n",
    "            batch_done = torch.FloatTensor(batch_done)\n",
    "    \n",
    "            current_q = rb_model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                #Same thing as without relay buffer, but max_next_q has slightly different dimensions which we need to adjust for\n",
    "                max_next_q = rb_model(batch_next_state).max(1)[0]\n",
    "                target_q = batch_reward + (gamma * max_next_q * (1 - batch_done))\n",
    "            \n",
    "            loss = nn.MSELoss()(current_q, target_q)        \n",
    "            rb_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rb_optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "    rb_results.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1630a96e-85ab-426a-a3d2-1d0436b2c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time from episode 0-10: 15.18\n",
      "\n",
      "Average Time from episode 100-110: 113.82\n",
      "\n",
      "Average Time from episode 200-210: 223.99\n",
      "\n",
      "Average Time from episode 300-310: 453.87\n",
      "\n",
      "Average Time from episode 400-410: 240.8\n",
      "\n",
      "Average Time from episode 500-510: 341.47\n",
      "\n",
      "Average Time from episode 600-610: 281.6\n",
      "\n",
      "Average Time from episode 700-710: 185.08\n",
      "\n",
      "Average Time from episode 800-810: 359.94\n",
      "\n",
      "Average Time from episode 900-910: 183.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing replay buffer results\n",
    "\n",
    "for i in range(0,1000,100):\n",
    "    avg = sum(rb_results[i:i+100]) / len(rb_results[i:i+100])\n",
    "    print(f\"Average Time from episode {i}-{i+10}: {avg}\\n\")\n",
    "#Results can become worse over time, as bad performance can affect agent internal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b26e2-033a-49fa-81f9-05963e62017b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b1aee43-f82b-4f27-a10c-af0a8d2677e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving replay buffer into a video file\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "rb_video_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "rb_video_env = RecordVideo(rb_video_env, video_folder=\"./videos\", episode_trigger=lambda x: True, name_prefix=\"replay_buffer_dqn_cartpole\")\n",
    "\n",
    "state, _ = rb_video_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state)\n",
    "    action = torch.argmax(rb_model(state_tensor)).item()\n",
    "    state, reward, terminated, truncated, _ = rb_video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "rb_video_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3c402b-9d7e-4637-9144-41150a08a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 1000/1000 [10:11<00:00,  1.63ep/s]\n"
     ]
    }
   ],
   "source": [
    "#Implementing Target Network\n",
    "#By having two DQN nets, we can mitigate the moving goalpost problem where the goal and weights are updated in a singular network\n",
    "#This can help solve our problem where our agent was optimizing going only right\n",
    "#Using same code, but adding comments to new lines where we implement target network\n",
    "tn_env = gym.make(\"CartPole-v1\")\n",
    "tn_state_size = tn_env.observation_space.shape[0]\n",
    "tn_action_size = tn_env.action_space.n\n",
    "policy = DQN(tn_state_size,tn_action_size)\n",
    "target = DQN(tn_state_size,tn_action_size)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(),lr = 0.001)\n",
    "gamma = 0.99\n",
    "\n",
    "memory = ReplayBuffer(10000)\n",
    "batch_size = 32\n",
    "total_steps = 0\n",
    "tn_results = []\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "\n",
    "for episode in tqdm(range(1000), desc=\"Training Episodes\", unit=\"ep\"):\n",
    "    state, _ = tn_env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        total_steps += 1\n",
    "        \n",
    "        q_values = policy(state)\n",
    "        if random.uniform(0, 1) > epsilon:  #Use decaying epsilon to discourage exploration as time increases, hoping that the agent knows more for exploitation\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = tn_env.action_space.sample()\n",
    " \n",
    "        next_state, reward, termination, truncation, _ = tn_env.step(action)\n",
    "        done = termination or truncation\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "        #Check if buffer is greater than batch size\n",
    "        if len(memory) >= batch_size:\n",
    "            #Retrieve 32 (from batch_size) entries from replay buffer\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = memory.sample(batch_size)        \n",
    "\n",
    "            #Stack states so that the neural network can process all states in the batch at one time\n",
    "            batch_state = torch.stack(batch_state)\n",
    "            batch_next_state = torch.stack(batch_next_state)\n",
    "    \n",
    "            #Convert reward and done into tensors for neural network processing\n",
    "            batch_reward = torch.FloatTensor(batch_reward)\n",
    "            batch_action = torch.LongTensor(batch_action)\n",
    "            batch_done = torch.FloatTensor(batch_done)\n",
    "    \n",
    "            current_q = policy(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                #Same thing as without relay buffer, but max_next_q has slightly different dimensions which we need to adjust for\n",
    "                max_next_q = target(batch_next_state).max(1)[0]\n",
    "                target_q = batch_reward + (gamma * max_next_q * (1 - batch_done))\n",
    "            \n",
    "            loss = nn.MSELoss()(current_q, target_q)        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #We still need to update our target network so that it doesn't become outdated and drag down the policy network\n",
    "            #However, we just update it at a slower pace. In this case every 100 steps\n",
    "            if total_steps % 50 == 0:\n",
    "                target.load_state_dict(policy.state_dict())\n",
    "        \n",
    "        state = next_state\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "    tn_results.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d7bc92e-7547-44f0-a7e1-1e51bb2de885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time from episode 0-10: 19.65\n",
      "\n",
      "Average Time from episode 10-20: 21.91\n",
      "\n",
      "Average Time from episode 20-30: 25.98\n",
      "\n",
      "Average Time from episode 30-40: 29.94\n",
      "\n",
      "Average Time from episode 40-50: 37.08\n",
      "\n",
      "Average Time from episode 50-60: 52.28\n",
      "\n",
      "Average Time from episode 60-70: 85.85\n",
      "\n",
      "Average Time from episode 70-80: 125.21\n",
      "\n",
      "Average Time from episode 80-90: 170.49\n",
      "\n",
      "Average Time from episode 90-100: 214.65\n",
      "\n",
      "Average Time from episode 100-110: 260.97\n",
      "\n",
      "Average Time from episode 110-120: 302.51\n",
      "\n",
      "Average Time from episode 120-130: 328.96\n",
      "\n",
      "Average Time from episode 130-140: 348.29\n",
      "\n",
      "Average Time from episode 140-150: 377.1\n",
      "\n",
      "Average Time from episode 150-160: 406.43\n",
      "\n",
      "Average Time from episode 160-170: 412.33\n",
      "\n",
      "Average Time from episode 170-180: 414.22\n",
      "\n",
      "Average Time from episode 180-190: 412.65\n",
      "\n",
      "Average Time from episode 190-200: 412.98\n",
      "\n",
      "Average Time from episode 200-210: 414.13\n",
      "\n",
      "Average Time from episode 210-220: 410.45\n",
      "\n",
      "Average Time from episode 220-230: 394.46\n",
      "\n",
      "Average Time from episode 230-240: 372.85\n",
      "\n",
      "Average Time from episode 240-250: 338.14\n",
      "\n",
      "Average Time from episode 250-260: 293.67\n",
      "\n",
      "Average Time from episode 260-270: 253.87\n",
      "\n",
      "Average Time from episode 270-280: 211.32\n",
      "\n",
      "Average Time from episode 280-290: 166.78\n",
      "\n",
      "Average Time from episode 290-300: 170.06\n",
      "\n",
      "Average Time from episode 300-310: 170.06\n",
      "\n",
      "Average Time from episode 310-320: 177.99\n",
      "\n",
      "Average Time from episode 320-330: 212.35\n",
      "\n",
      "Average Time from episode 330-340: 259.63\n",
      "\n",
      "Average Time from episode 340-350: 307.29\n",
      "\n",
      "Average Time from episode 350-360: 354.92\n",
      "\n",
      "Average Time from episode 360-370: 393.17\n",
      "\n",
      "Average Time from episode 370-380: 439.94\n",
      "\n",
      "Average Time from episode 380-390: 487.73\n",
      "\n",
      "Average Time from episode 390-400: 484.25\n",
      "\n",
      "Average Time from episode 400-410: 482.59\n",
      "\n",
      "Average Time from episode 410-420: 476.7\n",
      "\n",
      "Average Time from episode 420-430: 471.63\n",
      "\n",
      "Average Time from episode 430-440: 458.27\n",
      "\n",
      "Average Time from episode 440-450: 436.91\n",
      "\n",
      "Average Time from episode 450-460: 413.9\n",
      "\n",
      "Average Time from episode 460-470: 399.54\n",
      "\n",
      "Average Time from episode 470-480: 384.73\n",
      "\n",
      "Average Time from episode 480-490: 371.02\n",
      "\n",
      "Average Time from episode 490-500: 358.74\n",
      "\n",
      "Average Time from episode 500-510: 346.47\n",
      "\n",
      "Average Time from episode 510-520: 338.95\n",
      "\n",
      "Average Time from episode 520-530: 323.44\n",
      "\n",
      "Average Time from episode 530-540: 321.18\n",
      "\n",
      "Average Time from episode 540-550: 334.43\n",
      "\n",
      "Average Time from episode 550-560: 345.27\n",
      "\n",
      "Average Time from episode 560-570: 353.56\n",
      "\n",
      "Average Time from episode 570-580: 355.77\n",
      "\n",
      "Average Time from episode 580-590: 358.39\n",
      "\n",
      "Average Time from episode 590-600: 363.54\n",
      "\n",
      "Average Time from episode 600-610: 374.54\n",
      "\n",
      "Average Time from episode 610-620: 367.45\n",
      "\n",
      "Average Time from episode 620-630: 372.54\n",
      "\n",
      "Average Time from episode 630-640: 380.67\n",
      "\n",
      "Average Time from episode 640-650: 369.35\n",
      "\n",
      "Average Time from episode 650-660: 360.36\n",
      "\n",
      "Average Time from episode 660-670: 347.83\n",
      "\n",
      "Average Time from episode 670-680: 335.85\n",
      "\n",
      "Average Time from episode 680-690: 317.81\n",
      "\n",
      "Average Time from episode 690-700: 293.39\n",
      "\n",
      "Average Time from episode 700-710: 276.14\n",
      "\n",
      "Average Time from episode 710-720: 285.22\n",
      "\n",
      "Average Time from episode 720-730: 297.09\n",
      "\n",
      "Average Time from episode 730-740: 292.7\n",
      "\n",
      "Average Time from episode 740-750: 304.21\n",
      "\n",
      "Average Time from episode 750-760: 326.68\n",
      "\n",
      "Average Time from episode 760-770: 352.81\n",
      "\n",
      "Average Time from episode 770-780: 375.2\n",
      "\n",
      "Average Time from episode 780-790: 402.4\n",
      "\n",
      "Average Time from episode 790-800: 427.71\n",
      "\n",
      "Average Time from episode 800-810: 435.08\n",
      "\n",
      "Average Time from episode 810-820: 440.97\n",
      "\n",
      "Average Time from episode 820-830: 443.31\n",
      "\n",
      "Average Time from episode 830-840: 455.18\n",
      "\n",
      "Average Time from episode 840-850: 451.22\n",
      "\n",
      "Average Time from episode 850-860: 451.22\n",
      "\n",
      "Average Time from episode 860-870: 448.06\n",
      "\n",
      "Average Time from episode 870-880: 446.72\n",
      "\n",
      "Average Time from episode 880-890: 436.23\n",
      "\n",
      "Average Time from episode 890-900: 445.95\n",
      "\n",
      "Average Time from episode 900-910: 458.76\n",
      "\n",
      "Average Time from episode 910-920: 460.3222222222222\n",
      "\n",
      "Average Time from episode 920-930: 456.9625\n",
      "\n",
      "Average Time from episode 930-940: 450.8285714285714\n",
      "\n",
      "Average Time from episode 940-950: 462.43333333333334\n",
      "\n",
      "Average Time from episode 950-960: 454.92\n",
      "\n",
      "Average Time from episode 960-970: 451.55\n",
      "\n",
      "Average Time from episode 970-980: 454.76666666666665\n",
      "\n",
      "Average Time from episode 980-990: 500.0\n",
      "\n",
      "Average Time from episode 990-1000: 500.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing replay buffer results\n",
    "\n",
    "for i in range(0,1000,10):\n",
    "    avg = sum(tn_results[i:i+100]) / len(tn_results[i:i+100])\n",
    "    print(f\"Average Time from episode {i}-{i+10}: {avg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856994ca-3634-41e1-8cb3-aba95b51170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\isaac\\Projects\\RL\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#Saving replay buffer + target network into a video file\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "tn_video_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "tn_video_env = RecordVideo(tn_video_env, video_folder=\"./videos\", episode_trigger=lambda x: True, name_prefix=\"target_network_dqn_cartpole\")\n",
    "\n",
    "state, _ = tn_video_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state)\n",
    "    action = torch.argmax(policy(state_tensor)).item()\n",
    "    state, reward, terminated, truncated, _ = tn_video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "tn_video_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513a18d-34f2-4e3b-a319-a400f25a5e39",
   "metadata": {},
   "source": [
    "Notes and insights:\n",
    "1. More episodes doesn't necessarily mean higher performing agent\n",
    "2. At certain episode #s, simpler approaches seem to work better i.e. basic DQN vs relay buffer powered DQN\n",
    "\n",
    "Possible paths to explore:\n",
    "1. Different episode numbers for each method to determine peak effectiveness (episodes = n)\n",
    "2. Reducing or increasing batch size for replay buffer (batch_size = n)\n",
    "3. Reducing or increasing how frequent target network updates (1:n steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b32983-9c0d-4507-9654-618fa1bba1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
