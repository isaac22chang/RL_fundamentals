{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db00118-c66a-4cfc-b074-0b05b6c9a77f",
   "metadata": {},
   "source": [
    "Notebook covers:\n",
    "- Implementation of DQN using Gymnasium's Cart Pole v1 environment\n",
    "- Includes personal notes at each step to increase intuition and understanding of how reinforcement learning works\n",
    "- Timeline and reasoning of DQN Improvements (basic DQN -> implementing replay Buffer -> target network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ebe06f-ff1b-49db-bd8a-6c51a5ccf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed64c15-64e4-4a08-a2ad-b60c207b4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network as model\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim,64), #map # of states to 64 dimensions\n",
    "            nn.ReLU(), #introduce non-linearity by turning all negative values into 0\n",
    "            nn.Linear(64,action_size), #map 64 dimensions to # of actions\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188610b-8a86-450d-b598-31d25630e709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "model = DQN(state_size,action_size)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "gamma = 0.99\n",
    "\n",
    "results = []\n",
    "for episode in tqdm(range(1000), desc=\"Training Episodes\", unit=\"ep\"):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        q_values = model(state)\n",
    "        #Greedy epsilon\n",
    "        if random.uniform(0,1) > 0.1: #Set at 10% chance for exploration\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        next_state, reward, termination, truncation, _ = env.step(action)\n",
    "        done = termination or truncation\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        episode_reward += reward\n",
    "        \n",
    "        current_q = model(state)[action]\n",
    "        \n",
    "        #Make sure that we don't update weights of the actual model\n",
    "        with torch.no_grad():\n",
    "            max_next_q = torch.max(model(next_state)) #Given the current choice, we look at the next state to determine if we made the most informed choice\n",
    "            target_q = reward + (gamma * max_next_q * (1 - done)) #Since there's some discrepancy between current state and reality of next state, we update loss to make current closer to truth \n",
    "        loss = nn.MSELoss()(current_q, target_q)\n",
    "        #Note: Since loss is more about minimizing diff between reality and expected rewards, loss isn't a good rep of how good an agent performs\n",
    "\n",
    "        #Update loss given the information we have\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "    results.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88e4016e-aa5e-44dd-965b-136ad0ecf135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time from episode 0-100: 9.87\n",
      "\n",
      "Average Time from episode 100-200: 10.96\n",
      "\n",
      "Average Time from episode 200-300: 12.93\n",
      "\n",
      "Average Time from episode 300-400: 14.98\n",
      "\n",
      "Average Time from episode 400-500: 17.28\n",
      "\n",
      "Average Time from episode 500-600: 25.43\n",
      "\n",
      "Average Time from episode 600-700: 27.14\n",
      "\n",
      "Average Time from episode 700-800: 55.27\n",
      "\n",
      "Average Time from episode 800-900: 105.91\n",
      "\n",
      "Average Time from episode 900-1000: 130.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing results\n",
    "for i in range(0,1000,100):\n",
    "    avg = sum(results[i:i+100]) / len(results[i:i+100])\n",
    "    print(f\"Average Time from episode {i}-{i+100}: {avg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "959871df-ca35-4918-9c2a-04ecaa3c5451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\isaac\\Projects\\RL\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#Code to save a trained agent run into video folder\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "video_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "video_env = RecordVideo(video_env, video_folder=\"./videos\", episode_trigger=lambda x: True,name_prefix=\"basic_dqn_cartpole\")\n",
    "\n",
    "state, _ = video_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state)\n",
    "    action = torch.argmax(model(state_tensor)).item()\n",
    "    state, reward, terminated, truncated, _ = video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "video_env.close()\n",
    "\n",
    "#Notes:\n",
    "#Watching the agent, the agent just leans to the right or left\n",
    "#Due to sequential correlation, the agent leans to the right instead of trying to balance\n",
    "#One way to address this issue is to use replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f94928-b51a-480c-ab1a-62dc39bd936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "#Implementation of Replay Buffer\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self,size):\n",
    "        batch = random.sample(self.buffer,size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd6a671-2d4d-410d-a99f-a469e0101cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 1000/1000 [07:37<00:00,  2.19ep/s]\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayBuffer(10000)\n",
    "batch_size = 32\n",
    "rb_env = gym.make(\"CartPole-v1\")\n",
    "rb_state_size = rb_env.observation_space.shape[0]\n",
    "rb_action_size = rb_env.action_space.n\n",
    "rb_model = DQN(rb_state_size,rb_action_size)\n",
    "rb_optimizer = optim.Adam(rb_model.parameters(),lr = 0.001)\n",
    "gamma = 0.99\n",
    "\n",
    "rb_results = []\n",
    "for episode in tqdm(range(1000), desc=\"Training Episodes\", unit=\"ep\"):\n",
    "    state, _ = rb_env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        q_values = rb_model(state)\n",
    "        if random.uniform(0,1) > 0.1: #Set at 10% chance for exploration\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = rb_env.action_space.sample()\n",
    " \n",
    "        next_state, reward, termination, truncation, _ = rb_env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        \n",
    "        done = termination or truncation\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "\n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "        #Check if buffer is greater than batch size\n",
    "        if len(memory) >= batch_size:\n",
    "            #Retrieve 32 (from batch_size) entries from replay buffer\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = memory.sample(batch_size)        \n",
    "\n",
    "            #Stack states so that the neural network can process all states in the batch at one time\n",
    "            batch_state = torch.stack(batch_state)\n",
    "            batch_next_state = torch.stack(batch_next_state)\n",
    "    \n",
    "            #Convert reward and done into tensors for neural network processing\n",
    "            batch_reward = torch.FloatTensor(batch_reward)\n",
    "            batch_action = torch.LongTensor(batch_action)\n",
    "            batch_done = torch.FloatTensor(batch_done)\n",
    "    \n",
    "            current_q = rb_model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                #Same thing as without relay buffer, but max_next_q has slightly different dimensions which we need to adjust for\n",
    "                max_next_q = rb_model(batch_next_state).max(1)[0]\n",
    "                target_q = batch_reward + (gamma * max_next_q * (1 - batch_done))\n",
    "            \n",
    "            loss = nn.MSELoss()(current_q, target_q)        \n",
    "            rb_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rb_optimizer.step()\n",
    "        \n",
    "        state = next_state\n",
    "    rb_results.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1630a96e-85ab-426a-a3d2-1d0436b2c816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time from episode 0-100: 15.18\n",
      "\n",
      "Average Time from episode 100-200: 113.82\n",
      "\n",
      "Average Time from episode 200-300: 223.99\n",
      "\n",
      "Average Time from episode 300-400: 453.87\n",
      "\n",
      "Average Time from episode 400-500: 240.8\n",
      "\n",
      "Average Time from episode 500-600: 341.47\n",
      "\n",
      "Average Time from episode 600-700: 281.6\n",
      "\n",
      "Average Time from episode 700-800: 185.08\n",
      "\n",
      "Average Time from episode 800-900: 359.94\n",
      "\n",
      "Average Time from episode 900-1000: 183.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing replay buffer results\n",
    "\n",
    "for i in range(0,1000,100):\n",
    "    avg = sum(rb_results[i:i+100]) / len(rb_results[i:i+100])\n",
    "    print(f\"Average Time from episode {i}-{i+100}: {avg}\\n\")\n",
    "#Results can become worse over time, as bad performance can affect agent internal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b1aee43-f82b-4f27-a10c-af0a8d2677e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving replay buffer into a video file\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "rb_video_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "rb_video_env = RecordVideo(rb_video_env, video_folder=\"./videos\", episode_trigger=lambda x: True, name_prefix=\"replay_buffer_dqn_cartpole\")\n",
    "\n",
    "state, _ = rb_video_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state)\n",
    "    action = torch.argmax(rb_model(state_tensor)).item()\n",
    "    state, reward, terminated, truncated, _ = rb_video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "rb_video_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3c402b-9d7e-4637-9144-41150a08a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes: 100%|███████████████████████████████████████████████████████████| 1000/1000 [10:11<00:00,  1.63ep/s]\n"
     ]
    }
   ],
   "source": [
    "#Implementing Target Network\n",
    "#By having two DQN nets, we can mitigate the moving goalpost problem where the goal and weights are updated in a singular network\n",
    "#This can help solve our problem where our agent was optimizing going only right\n",
    "#Using same code, but adding comments to new lines where we implement target network\n",
    "tn_env = gym.make(\"CartPole-v1\")\n",
    "tn_state_size = tn_env.observation_space.shape[0]\n",
    "tn_action_size = tn_env.action_space.n\n",
    "policy = DQN(tn_state_size,tn_action_size)\n",
    "target = DQN(tn_state_size,tn_action_size)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "optimizer = optim.Adam(policy.parameters(),lr = 0.001)\n",
    "gamma = 0.99\n",
    "\n",
    "memory = ReplayBuffer(10000)\n",
    "batch_size = 32\n",
    "total_steps = 0\n",
    "tn_results = []\n",
    "\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "\n",
    "for episode in tqdm(range(1000), desc=\"Training Episodes\", unit=\"ep\"):\n",
    "    state, _ = tn_env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        total_steps += 1\n",
    "        \n",
    "        q_values = policy(state)\n",
    "        if random.uniform(0, 1) > epsilon:  #Use decaying epsilon to discourage exploration as time increases, hoping that the agent knows more for exploitation\n",
    "            action = torch.argmax(q_values).item()\n",
    "        else:\n",
    "            action = tn_env.action_space.sample()\n",
    " \n",
    "        next_state, reward, termination, truncation, _ = tn_env.step(action)\n",
    "        done = termination or truncation\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "        memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "        #Check if buffer is greater than batch size\n",
    "        if len(memory) >= batch_size:\n",
    "            #Retrieve 32 (from batch_size) entries from replay buffer\n",
    "            batch_state, batch_action, batch_reward, batch_next_state, batch_done = memory.sample(batch_size)        \n",
    "\n",
    "            #Stack states so that the neural network can process all states in the batch at one time\n",
    "            batch_state = torch.stack(batch_state)\n",
    "            batch_next_state = torch.stack(batch_next_state)\n",
    "    \n",
    "            #Convert reward and done into tensors for neural network processing\n",
    "            batch_reward = torch.FloatTensor(batch_reward)\n",
    "            batch_action = torch.LongTensor(batch_action)\n",
    "            batch_done = torch.FloatTensor(batch_done)\n",
    "    \n",
    "            current_q = policy(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "            with torch.no_grad():\n",
    "                #Same thing as without relay buffer, but max_next_q has slightly different dimensions which we need to adjust for\n",
    "                max_next_q = target(batch_next_state).max(1)[0]\n",
    "                target_q = batch_reward + (gamma * max_next_q * (1 - batch_done))\n",
    "            \n",
    "            loss = nn.MSELoss()(current_q, target_q)        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #We still need to update our target network so that it doesn't become outdated and drag down the policy network\n",
    "            #However, we just update it at a slower pace. In this case every 100 steps\n",
    "            if total_steps % 50 == 0:\n",
    "                target.load_state_dict(policy.state_dict())\n",
    "        \n",
    "        state = next_state\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "    tn_results.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d7bc92e-7547-44f0-a7e1-1e51bb2de885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time from episode 0-100: 19.65\n",
      "\n",
      "Average Time from episode 100-200: 260.97\n",
      "\n",
      "Average Time from episode 200-300: 414.13\n",
      "\n",
      "Average Time from episode 300-400: 170.06\n",
      "\n",
      "Average Time from episode 400-500: 482.59\n",
      "\n",
      "Average Time from episode 500-600: 346.47\n",
      "\n",
      "Average Time from episode 600-700: 374.54\n",
      "\n",
      "Average Time from episode 700-800: 276.14\n",
      "\n",
      "Average Time from episode 800-900: 435.08\n",
      "\n",
      "Average Time from episode 900-1000: 458.76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing replay buffer results\n",
    "\n",
    "for i in range(0,1000,100):\n",
    "    avg = sum(tn_results[i:i+100]) / len(tn_results[i:i+100])\n",
    "    print(f\"Average Time from episode {i}-{i+100}: {avg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856994ca-3634-41e1-8cb3-aba95b51170b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isaac\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at C:\\Users\\isaac\\Projects\\RL\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "#Saving replay buffer + target network into a video file\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "tn_video_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "tn_video_env = RecordVideo(tn_video_env, video_folder=\"./videos\", episode_trigger=lambda x: True, name_prefix=\"target_network_dqn_cartpole\")\n",
    "\n",
    "state, _ = tn_video_env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state_tensor = torch.FloatTensor(state)\n",
    "    action = torch.argmax(policy(state_tensor)).item()\n",
    "    state, reward, terminated, truncated, _ = tn_video_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "tn_video_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513a18d-34f2-4e3b-a319-a400f25a5e39",
   "metadata": {},
   "source": [
    "Notes and insights:\n",
    "1. More episodes doesn't necessarily mean higher performing agent\n",
    "2. At certain episode #s, simpler approaches seem to work better i.e. basic DQN vs relay buffer powered DQN\n",
    "\n",
    "Possible paths to explore:\n",
    "1. Different episode numbers for each method to determine peak effectiveness (episodes = n)\n",
    "2. Reducing or increasing batch size for replay buffer (batch_size = n)\n",
    "3. Reducing or increasing how frequent target network updates (1:n steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b32983-9c0d-4507-9654-618fa1bba1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
